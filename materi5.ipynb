{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week #3 - Data Extraction\n",
    "Data Pipeline Course - Sekolah Engineer - Pacmann Academy \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Data extraction is the first step in building a data pipeline, where data is gathered from various sources. This chapter covers various methods of extracting data using Python, including full and incremental extraction from a PostgreSQL database, extracting data from APIs, handling backfilling, extracting data from spreadsheets using Google Service API and data pipeline logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Description\n",
    "![title](pict/extract1.png)\n",
    "\n",
    "The company's transaction data is separated as follows:\n",
    "1. Order data from an API\n",
    "2. Customer and Product data in a PostgreSQL Database\n",
    "3. Details about product categories and subcategories in a spreadsheetadsheet\n",
    "\n",
    "Task:\n",
    "Perform data extraction with the following plan:\n",
    "1. Extraction will be performed iteratively every day, so only the latest data will be extracted.\n",
    "2. API Backfilling: For historical API data, backfilling will be performed to retrieve old data.\n",
    "3. Database Data Extraction: <br> \n",
    "    Steps: (1) Full Extraction, (2) Incremental Extraction\n",
    "4. Spreadsheet Data Extraction: <br>\n",
    "    Extraction from the spreadsheet will be performed similarly to the Full Extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOG\n",
    "A log is a record of events that occur during the execution of a data pipeline. It captures essential information about the processes and their status, making it easier to monitor, debug, and audit the pipeline operations. We will save our extraction log to a csv file with this format:\n",
    "\n",
    "<code>\n",
    "log_msg = { <br>\n",
    "            \"step\" : \"extraction | transformation | load\", <br>\n",
    "            \"status\": \"success | failed\", <br>\n",
    "            \"source\": \"spreadsheet | database | api\", <br>\n",
    "            \"table_name\": \"worksheet_name | table_name\", <br>\n",
    "            \"etl_date\": \"Current timestamp\" <br>\n",
    "        }\n",
    "</code>\n",
    "\n",
    "The log_to_csv function appends a log message to a CSV file. If the file does not exist, it creates the file and writes the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_csv(log_msg: dict, filename: str):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    # Define the column headers\n",
    "    headers = [\"step\", \"status\", \"source\", \"table_name\",\"etl_date\"]\n",
    "\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "\n",
    "        # Write the header only if the file doesn't exist\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Append the log message\n",
    "        writer.writerow(log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spreadsheet\n",
    "\n",
    "- Source: [link](https://docs.google.com/spreadsheets/d/1354yIiiX5peKRL4fbTC1aVA40bqgsxg3fA1zPP0e3uQ/edit?usp=drive_link)\n",
    "- Sheet: Category and Subcategory\n",
    "\n",
    "The team updates the data in the spreadsheet whenever there are new categories or subcategories. Downloading the data as a CSV file would complicate the process. Therefore, the preferred approach is to directly extract data from the spreadsheet using Google Services.\n",
    "\n",
    "By using Google Services, the data can be accessed programmatically, ensuring that the latest updates are always retrieved without the need for manual downloads. This method streamlines the data extraction process and keeps the workflow efficient.\n",
    "\n",
    "\n",
    "![title](pict/extract3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preparation\n",
    "To extract data directly from the spreadsheet, we need a ```credentials``` file to access the spreadsheet. Here are the steps to obtain credentials through ```Google Service```:\n",
    "1. Create a project and service account\n",
    "access: https://console.cloud.google.com <br>\n",
    "    a. Create New Project <br>\n",
    "    <img src='pict/extract4.png' width=\"800\"> <br>\n",
    "    b. Go to IAM & Admin > Service Accounts <br>\n",
    "    <img src='pict/extract5.png' width=\"800\"> <br>\n",
    "    c. Create service account > Create and Continue > Skip all fields in step 2 and 3 > Done <br>\n",
    "    <img src='pict/extract6.png' width=\"800\"> <br>\n",
    "    Here, we obtain an email address that we will use to access the spreadsheet file. <br>\n",
    "    example: data-pipeline@data-pipeline-427506.iam.gserviceaccount.com\t<br>\n",
    "\n",
    "\n",
    "2. Create JSON credentials file <br>\n",
    "    Obtain the credentials file to be used by our data pipeline project. <br>\n",
    "    a. Get it by clicking the three vertical dots button > Manage keys <br>\n",
    "    <img src='pict/extract7.png' width=\"800\"> <br>\n",
    "    b. Create New Key. ADD KEY > Create New Key > JSON. The file credentials will be downloaded automatically <br>\n",
    "    <img src='pict/extract8.png' width=\"800\"> <br> <br>\n",
    "\n",
    "\n",
    "3. Share your google sheet to the service account <br>\n",
    "    a. ```Duplicate this file on your Google Drive```: [link](https://docs.google.com/spreadsheets/d/1354yIiiX5peKRL4fbTC1aVA40bqgsxg3fA1zPP0e3uQ/edit?usp=drive_link) <br>\n",
    "    b. Add permissions for the email service <br>\n",
    "    <img src='pict/extract9.png' width=\"500\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Library dan Workflow\n",
    "The library used to access the spreadsheet is\n",
    "- gspread\n",
    "- oauth2client\n",
    "\n",
    "Workflow:\n",
    "Define Google Service Credential &rarr; Select the spreadsheet file to extract &rarr; Select the sheet to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gspread in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (6.1.2)\n",
      "Requirement already satisfied: google-auth>=1.12.0 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gspread) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gspread) (1.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth>=1.12.0->gspread) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth>=1.12.0->gspread) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth>=1.12.0->gspread) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: C:\\Users\\ihdarsyd\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: oauth2client in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.1.3)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from oauth2client) (0.22.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from oauth2client) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from oauth2client) (0.4.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from oauth2client) (4.9)\n",
      "Requirement already satisfied: six>=1.6.1 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from oauth2client) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\ihdarsyd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httplib2>=0.9.1->oauth2client) (3.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: C:\\Users\\ihdarsyd\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gspread\n",
    "!pip install oauth2client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import gspread\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. File .env\n",
    "Save the credential.json file information obtained from the Google Service created previously\n",
    "\n",
    "example:\n",
    "```\n",
    "CRED_PATH = 'C:/Data Pipeline/extract/creds/data-pipeline-427506-50d868a444ee.json'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "\n",
    "CRED_PATH = os.getenv(\"CRED_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Buat ```Function auth_gspread()```\n",
    "This function aims to define the credentials needed to access the spreadsheet.\n",
    "- The scope defines the permissions your application will request. In this case, the scope includes access to Google Sheets (https://spreadsheets.google.com/feeds) and Google Drive (https://www.googleapis.com/auth/drive).\n",
    "- ServiceAccountCredentials.from_json_keyfile_name is a method from the oauth2client library. It reads the service account credentials from a JSON key file specified by CRED_PATH.\n",
    "- gspread.authorize(credentials) takes the credentials object created in the previous step and returns a gspread client object (gc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_gspread():\n",
    "    scope = ['https://spreadsheets.google.com/feeds',\n",
    "             'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "    #Define your credentials\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(CRED_PATH, scope) # Your json file here\n",
    "\n",
    "    gc = gspread.authorize(credentials)\n",
    "\n",
    "    return gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Create ```Function init_key_file```\n",
    "The key_file is a unique identifier for the Google Sheets file.\n",
    "Access to the spreadsheet file is obtained using the key from that file.\n",
    "\n",
    "Example link:  ```https://docs.google.com/spreadsheets/d/1354yIiiX5peKRL4fbTC1aVA40bqgsxg3fA1zPP0e3uQ/edit?gid=83932574#gid=83932574```\n",
    "\n",
    "The key from that file: ```1354yIiiX5peKRL4fbTC1aVA40bqgsxg3fA1zPP0e3uQ```\n",
    "-  ```auth_gspread()``` to authenticate and authorize access to Google Sheets. \n",
    "- The ```gc.open_by_key(key_file)``` method uses the gspread client object to open the Google Sheets file identified by the key_file argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_key_file(key_file:str):\n",
    "    #define credentials to open the file\n",
    "    gc = auth_gspread()\n",
    "    \n",
    "    #open spreadsheet file by key\n",
    "    sheet_result = gc.open_by_key(key_file)\n",
    "    \n",
    "    return sheet_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Function ```extract_spreadsheet```  <br>\n",
    "The extract_spreadsheet function is used to retrieve data from a specific sheet. <br>\n",
    "- ```worksheet``` Function of the Spreadsheet object to access a specific worksheet by its name, worksheet_name.\n",
    "- The ```get_all_values``` Function of the Worksheet object retrieves all the values from the worksheet as a list of lists (where each inner list represents a row of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sheet(key_file:str, worksheet_name: str) -> pd.DataFrame:\n",
    "    # init sheet\n",
    "    sheet_result = init_key_file(key_file)\n",
    "    \n",
    "    worksheet_result = sheet_result.worksheet(worksheet_name)\n",
    "    \n",
    "    df_result = pd.DataFrame(worksheet_result.get_all_values())\n",
    "    \n",
    "    # set first rows as columns\n",
    "    df_result.columns = df_result.iloc[0]\n",
    "    \n",
    "    # get all the rest of the values\n",
    "    df_result = df_result[1:].copy()\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Perform Data Extraction <br>\n",
    "Extract data from the \"category\" and \"subcategory\" sheets and save the log information to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spreadsheet(worksheet_name: str, key_file: str):\n",
    "\n",
    "    try:\n",
    "        # extract data\n",
    "        df_data = extract_sheet(worksheet_name = worksheet_name,\n",
    "                                    key_file = key_file)\n",
    "        \n",
    "        # success log message\n",
    "        log_msg = {\n",
    "            \"step\" : \"extraction\",\n",
    "            \"status\": \"success\",\n",
    "            \"source\": \"spreadsheet\",\n",
    "            \"table_name\": worksheet_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # fail log message\n",
    "        log_msg = {\n",
    "            \"step\" : \"extraction\",\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"spreadsheet\",\n",
    "            \"table_name\": worksheet_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "    finally:\n",
    "        # load log to csv file\n",
    "        log_to_csv(log_msg, 'log.csv')\n",
    "        \n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_CATEGORY  = os.getenv(\"KEY_CATEGORY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_category = extract_spreadsheet(worksheet_name = 'category',\n",
    "                                    key_file = KEY_CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Computers&amp;Accessories</td>\n",
       "      <td>Computers&amp;Accessories is Skill final here skin...</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Electronics is Letter offer probably state org...</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MusicalInstruments</td>\n",
       "      <td>MusicalInstruments is Above without but federa...</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>OfficeProducts</td>\n",
       "      <td>OfficeProducts is Letter participant lot indic...</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>HomeImprovement</td>\n",
       "      <td>HomeImprovement is Meeting senior student win ...</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Toys&amp;Games</td>\n",
       "      <td>Toys&amp;Games is Local summer prevent authority h...</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Car&amp;Motorbike</td>\n",
       "      <td>Car&amp;Motorbike is Big people role me play onto.</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Health&amp;PersonalCare</td>\n",
       "      <td>Health&amp;PersonalCare is Stand response prove co...</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>Home and Kitchen</td>\n",
       "      <td>Home&amp;Kitchen is Service discussion again sea a...</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "      <td>2021-01-01 0:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 category_id                   name  \\\n",
       "1           1  Computers&Accessories   \n",
       "2           2            Electronics   \n",
       "3           3     MusicalInstruments   \n",
       "4           4         OfficeProducts   \n",
       "5           6        HomeImprovement   \n",
       "6           7             Toys&Games   \n",
       "7           8          Car&Motorbike   \n",
       "8           9    Health&PersonalCare   \n",
       "9           5       Home and Kitchen   \n",
       "\n",
       "0                                        description          created_at  \\\n",
       "1  Computers&Accessories is Skill final here skin...  2021-01-01 0:00:00   \n",
       "2  Electronics is Letter offer probably state org...  2021-01-01 0:00:00   \n",
       "3  MusicalInstruments is Above without but federa...  2021-01-01 0:00:00   \n",
       "4  OfficeProducts is Letter participant lot indic...  2021-01-01 0:00:00   \n",
       "5  HomeImprovement is Meeting senior student win ...  2021-01-01 0:00:00   \n",
       "6  Toys&Games is Local summer prevent authority h...  2021-01-01 0:00:00   \n",
       "7     Car&Motorbike is Big people role me play onto.  2021-01-01 0:00:00   \n",
       "8  Health&PersonalCare is Stand response prove co...  2021-01-01 0:00:00   \n",
       "9  Home&Kitchen is Service discussion again sea a...  2021-01-01 0:00:00   \n",
       "\n",
       "0          updated_at  \n",
       "1  2021-01-01 0:00:00  \n",
       "2  2021-01-01 0:00:00  \n",
       "3  2021-01-01 0:00:00  \n",
       "4  2021-01-01 0:00:00  \n",
       "5  2021-01-01 0:00:00  \n",
       "6  2021-01-01 0:00:00  \n",
       "7  2021-01-01 0:00:00  \n",
       "8  2021-01-01 0:00:00  \n",
       "9  2021-01-01 0:00:00  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database\n",
    "\n",
    "Source: Table product and customer\n",
    "[Link](https://drive.google.com/drive/folders/1ED0sg2AZNH_Kl5Pb1cBUufnPCphpM21R)\n",
    "\n",
    "\n",
    "Process:\n",
    "- Full Extraction: Retrieve all existing data from the tables.\n",
    "- Insert New Data: Insert new records with the current create date.\n",
    "- Incremental Extraction: Retrieve newly added data since the last extraction process.\n",
    "\n",
    "This process involves extracting data comprehensively, inserting new records with updated timestamps, and ensuring only new data is extracted incrementally in subsequent runs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data From PostgreSQL\n",
    "\n",
    "Function Steps:\n",
    "1. Establish Database Connection: Connects to a PostgreSQL database named mini_order.\n",
    "2. Read Log Data: Reads existing log data from log.csv to determine the last successful extraction timestamp (etl_date).\n",
    "3. Initial Load or Incremental Extraction:\n",
    "    - If no previous extraction has been recorded (etl_date is empty), set etl_date to '1111-01-01' indicating the initial load.\n",
    "    - Otherwise, retrieve data added since the last successful extraction (etl_date).\n",
    "4. Query Execution: Constructs a SQL query to select all columns from the specified table_name where created_at is greater than etl_date.\n",
    "5. Data Extraction: Executes the SQL query using pd.read_sql to fetch the data into a Pandas DataFrame (df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_database(table_name: str): \n",
    "    \n",
    "    try:\n",
    "        # create connection to database\n",
    "        conn = create_engine(\"postgresql://postgres:aku@localhost/mini_order\")\n",
    "\n",
    "        log = pd.read_csv(\"log.csv\")\n",
    "\n",
    "        # Get date from previous process\n",
    "        condition = (\n",
    "            (log['step'] == 'extraction') &\n",
    "            (log['status'] == 'success') &\n",
    "            (log['source'] == 'database') &\n",
    "            (log['table_name'] == table_name)\n",
    "        )\n",
    "\n",
    "        # Apply the filter\n",
    "        etl_date = log[condition]['etl_date']\n",
    "\n",
    "        # If no previous extraction has been recorded (etl_date is empty), set etl_date to '1111-01-01' indicating the initial load.\n",
    "        # Otherwise, retrieve data added since the last successful extraction (etl_date).\n",
    "        if(etl_date.empty):\n",
    "            etl_date = '1111-01-01'\n",
    "        else:\n",
    "            etl_date = max(etl_date)\n",
    "\n",
    "        # Constructs a SQL query to select all columns from the specified table_name where created_at is greater than etl_date.\n",
    "        query = f\"SELECT * FROM {table_name} WHERE created_at > %s::timestamp\"\n",
    "\n",
    "        # Execute the query with pd.read_sql\n",
    "        df = pd.read_sql(sql=query, con=conn, params=(etl_date,))\n",
    "        log_msg = {\n",
    "                \"step\" : \"extraction\",\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"database\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"extraction\",\n",
    "            \"status\": \"failed\",\n",
    "            \"source\": \"database\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "        }\n",
    "    finally:\n",
    "        log_to_csv(log_msg, 'log.csv')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer = extract_database(table_name=\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>address</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [customer_id, first_name, last_name, email, phone, address, created_at, updated_at]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Input New Data\n",
    "Input a new data record, then try extracting it to get the new data (without the old data that has been extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(data:pd.DataFrame ,table_name:str):\n",
    "    conn = create_engine(\"postgresql://postgres:aku@localhost/mini_order\")\n",
    "\n",
    "    data.to_sql(table_name, con=conn, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    \"customer_id\":[1001],\n",
    "    \"first_name\": [\"Emma\"],\n",
    "    \"last_name\": [\"Watson\"],\n",
    "    \"email\": [\"emmawatson@mailcom\"],\n",
    "    \"phone\": [\"639-601-6489\"],\n",
    "    \"address\": [\"0682 Davis Mount North Ryan, DE 34214\"]\n",
    "})\n",
    "\n",
    "insert_data(data,'customer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer = extract_database(table_name=\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>address</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Emma</td>\n",
       "      <td>Watson</td>\n",
       "      <td>emmawatson@mailcom</td>\n",
       "      <td>639-601-6489</td>\n",
       "      <td>0682 Davis Mount North Ryan, DE 34214</td>\n",
       "      <td>2024-07-02 16:44:46.245116</td>\n",
       "      <td>2024-07-02 16:44:46.245116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id first_name last_name               email         phone  \\\n",
       "0         1001       Emma    Watson  emmawatson@mailcom  639-601-6489   \n",
       "\n",
       "                                 address                 created_at  \\\n",
       "0  0682 Davis Mount North Ryan, DE 34214 2024-07-02 16:44:46.245116   \n",
       "\n",
       "                  updated_at  \n",
       "0 2024-07-02 16:44:46.245116  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "\n",
    "API data extraction involves retrieving data from an external source or service using API requests. This process is essential for integrating real-time or periodic updates into your data pipeline.\n",
    "\n",
    "If an API sends data daily and we want to extract data for each day, we simply specify the date parameter of the API with the current date. But what if we want to retrieve data from previous days? This is where ```backfilling``` comes into play. <br>\n",
    "Backfilling refers to the process of retrieving historical data that was missed during regular extraction intervals. This is typically necessary when integrating new data sources or when there were interruptions in data extraction. <br>\n",
    "\n",
    "Use Case: <br>\n",
    "- Daily Extraction: Set the date parameter of the API to the current date for daily extraction of data. <br>\n",
    "- Backfilling: Specify a date range in the date parameter to retrieve historical data that may have been missed in previous extractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Data API\n",
    "\n",
    "source: https://api-order-teal.vercel.app/api/dummydata <br>\n",
    "parameter:\n",
    "- start_date\n",
    "- end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_api(link_api:str, list_parameter:dict, data_name):\n",
    "    try:\n",
    "        # Establish connection to API\n",
    "        resp = requests.get(link_api, params=list_parameter)\n",
    "\n",
    "        # Parse the response JSON\n",
    "        raw_response = resp.json()\n",
    "\n",
    "        # Convert the JSON data to a pandas DataFrame\n",
    "        df_api = pd.DataFrame(raw_response)\n",
    "\n",
    "        # create success log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"extraction\",\n",
    "                \"status\": \"success\",\n",
    "                \"source\": \"api\",\n",
    "                \"table_name\": data_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "        return df_api\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while making the API request: {e}\")\n",
    "\n",
    "        # create fail log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"extraction\",\n",
    "                \"status\": \"failed\",\n",
    "                \"source\": \"api\",\n",
    "                \"table_name\": data_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred while parsing the response JSON: {e}\")\n",
    "\n",
    "        # create fail log message\n",
    "        log_msg = {\n",
    "                \"step\" : \"extraction\",\n",
    "                \"status\": \"failed\",\n",
    "                \"source\": \"api\",\n",
    "                \"table_name\": data_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    finally:\n",
    "        log_to_csv(log_msg, 'log.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data API (Backfilling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m link_api \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api-order-teal.vercel.app/api/dummydata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m list_parameter \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2020-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-01-02\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m }\n\u001b[1;32m----> 8\u001b[0m df_bacfilling \u001b[38;5;241m=\u001b[39m \u001b[43mextract_api\u001b[49m(link_api, list_parameter, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_api' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract data from 2020-01-01 to 2024-01-01\n",
    "link_api = \"https://api-order-teal.vercel.app/api/dummydata\"\n",
    "list_parameter = {\n",
    "    \"start_date\": \"2020-01-01\",\n",
    "    \"end_date\": \"2024-01-01\"\n",
    "}\n",
    "\n",
    "df_bacfilling = extract_api(link_api, list_parameter, \"order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>order_id</th>\n",
       "      <th>price</th>\n",
       "      <th>product_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>status</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-30 00:00:00.000</td>\n",
       "      <td>697</td>\n",
       "      <td>2022-01-30 00:00:00.000</td>\n",
       "      <td>IINI91PP812</td>\n",
       "      <td>1599.0</td>\n",
       "      <td>B08ZN4B121</td>\n",
       "      <td>7</td>\n",
       "      <td>Success</td>\n",
       "      <td>2022-01-30 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-30 00:00:00.000</td>\n",
       "      <td>697</td>\n",
       "      <td>2022-01-30 00:00:00.000</td>\n",
       "      <td>IINI91PP812</td>\n",
       "      <td>999.0</td>\n",
       "      <td>B0B94JPY2N</td>\n",
       "      <td>13</td>\n",
       "      <td>Success</td>\n",
       "      <td>2022-01-30 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-30 00:00:00.000</td>\n",
       "      <td>697</td>\n",
       "      <td>2022-01-30 00:00:00.000</td>\n",
       "      <td>IINI91PP812</td>\n",
       "      <td>299.0</td>\n",
       "      <td>B07MP21WJD</td>\n",
       "      <td>9</td>\n",
       "      <td>Success</td>\n",
       "      <td>2022-01-30 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-30 00:00:00.000</td>\n",
       "      <td>697</td>\n",
       "      <td>2022-01-30 00:00:00.000</td>\n",
       "      <td>IINI91PP812</td>\n",
       "      <td>999.0</td>\n",
       "      <td>B08G43CCLC</td>\n",
       "      <td>9</td>\n",
       "      <td>Success</td>\n",
       "      <td>2022-01-30 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-03 00:00:00.000</td>\n",
       "      <td>172</td>\n",
       "      <td>2021-01-03 00:00:00.000</td>\n",
       "      <td>ONNA03MN757</td>\n",
       "      <td>3999.0</td>\n",
       "      <td>B0B217Z5VK</td>\n",
       "      <td>5</td>\n",
       "      <td>Success</td>\n",
       "      <td>2021-01-03 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>2021-04-24 00:00:00.000</td>\n",
       "      <td>639</td>\n",
       "      <td>2021-04-24 00:00:00.000</td>\n",
       "      <td>AANA44AN436</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>B07NKNBTT3</td>\n",
       "      <td>1</td>\n",
       "      <td>Success</td>\n",
       "      <td>2021-04-24 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3625</th>\n",
       "      <td>2022-10-10 00:00:00.000</td>\n",
       "      <td>529</td>\n",
       "      <td>2022-10-10 00:00:00.000</td>\n",
       "      <td>IAAC58MO380</td>\n",
       "      <td>1499.0</td>\n",
       "      <td>B0083T231O</td>\n",
       "      <td>15</td>\n",
       "      <td>Success</td>\n",
       "      <td>2022-10-10 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626</th>\n",
       "      <td>2022-10-10 00:00:00.000</td>\n",
       "      <td>529</td>\n",
       "      <td>2022-10-10 00:00:00.000</td>\n",
       "      <td>IAAC58MO380</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>B07VZYMQNZ</td>\n",
       "      <td>4</td>\n",
       "      <td>Success</td>\n",
       "      <td>2022-10-10 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3627</th>\n",
       "      <td>2022-10-10 00:00:00.000</td>\n",
       "      <td>529</td>\n",
       "      <td>2022-10-10 00:00:00.000</td>\n",
       "      <td>IAAC58MO380</td>\n",
       "      <td>670.0</td>\n",
       "      <td>B09PTT8DZF</td>\n",
       "      <td>10</td>\n",
       "      <td>Success</td>\n",
       "      <td>2022-10-10 00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3628</th>\n",
       "      <td>2022-12-31 00:00:00.000</td>\n",
       "      <td>989</td>\n",
       "      <td>2022-12-31 00:00:00.000</td>\n",
       "      <td>ANAC00PM416</td>\n",
       "      <td>699.0</td>\n",
       "      <td>B07JF9B592</td>\n",
       "      <td>15</td>\n",
       "      <td>Success</td>\n",
       "      <td>2022-12-31 00:00:00.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3629 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   created_at  customer_id               order_date  \\\n",
       "0     2022-01-30 00:00:00.000          697  2022-01-30 00:00:00.000   \n",
       "1     2022-01-30 00:00:00.000          697  2022-01-30 00:00:00.000   \n",
       "2     2022-01-30 00:00:00.000          697  2022-01-30 00:00:00.000   \n",
       "3     2022-01-30 00:00:00.000          697  2022-01-30 00:00:00.000   \n",
       "4     2021-01-03 00:00:00.000          172  2021-01-03 00:00:00.000   \n",
       "...                       ...          ...                      ...   \n",
       "3624  2021-04-24 00:00:00.000          639  2021-04-24 00:00:00.000   \n",
       "3625  2022-10-10 00:00:00.000          529  2022-10-10 00:00:00.000   \n",
       "3626  2022-10-10 00:00:00.000          529  2022-10-10 00:00:00.000   \n",
       "3627  2022-10-10 00:00:00.000          529  2022-10-10 00:00:00.000   \n",
       "3628  2022-12-31 00:00:00.000          989  2022-12-31 00:00:00.000   \n",
       "\n",
       "         order_id   price  product_id  quantity   status  \\\n",
       "0     IINI91PP812  1599.0  B08ZN4B121         7  Success   \n",
       "1     IINI91PP812   999.0  B0B94JPY2N        13  Success   \n",
       "2     IINI91PP812   299.0  B07MP21WJD         9  Success   \n",
       "3     IINI91PP812   999.0  B08G43CCLC         9  Success   \n",
       "4     ONNA03MN757  3999.0  B0B217Z5VK         5  Success   \n",
       "...           ...     ...         ...       ...      ...   \n",
       "3624  AANA44AN436  1230.0  B07NKNBTT3         1  Success   \n",
       "3625  IAAC58MO380  1499.0  B0083T231O        15  Success   \n",
       "3626  IAAC58MO380  1440.0  B07VZYMQNZ         4  Success   \n",
       "3627  IAAC58MO380   670.0  B09PTT8DZF        10  Success   \n",
       "3628  ANAC00PM416   699.0  B07JF9B592        15  Success   \n",
       "\n",
       "                   updated_at  \n",
       "0     2022-01-30 00:00:00.000  \n",
       "1     2022-01-30 00:00:00.000  \n",
       "2     2022-01-30 00:00:00.000  \n",
       "3     2022-01-30 00:00:00.000  \n",
       "4     2021-01-03 00:00:00.000  \n",
       "...                       ...  \n",
       "3624  2021-04-24 00:00:00.000  \n",
       "3625  2022-10-10 00:00:00.000  \n",
       "3626  2022-10-10 00:00:00.000  \n",
       "3627  2022-10-10 00:00:00.000  \n",
       "3628  2022-12-31 00:00:00.000  \n",
       "\n",
       "[3629 rows x 9 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bacfilling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Today Data API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "link_api = \"https://api-order-teal.vercel.app/api/dummydata\"\n",
    "list_parameter = {\n",
    "    \"start_date\": current_date,\n",
    "    \"end_date\": current_date\n",
    "}\n",
    "\n",
    "df_api = extract_api(link_api, list_parameter, \"order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_api\n",
    "# There is no new data available at this time (empty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link git repository: https://github.com/Kurikulum-Sekolah-Pacmann/ingestion_data_pipeline.git"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
